{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Activation, Input, Dropout, Add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2 Tests.ipynb               nohup.out\r\n",
      "A2 model evaluation.ipynb    predicate-matrix-binary.txt\r\n",
      "HW2.pdf                      predicates.txt\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m/                 sample_submission.txt\r\n",
      "classes.txt                  test_images.txt\r\n",
      "eval_awa.py                  testclasses.txt\r\n",
      "\u001b[1m\u001b[34mimages_128x128\u001b[m\u001b[m/              train_model_1.py\r\n",
      "model_evaluate.py            trainclasses.txt\r\n",
      "\u001b[1m\u001b[34mmodels\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chimpanzee\r\n",
      "giant+panda\r\n",
      "leopard\r\n",
      "persian+cat\r\n",
      "pig\r\n",
      "hippopotamus\r\n",
      "humpback+whale\r\n",
      "raccoon\r\n",
      "rat\r\n",
      "seal\r\n"
     ]
    }
   ],
   "source": [
    "cat testclasses.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(data_dir):\n",
    "    animal_to_imgs = {}\n",
    "    for animal_name in os.listdir(data_dir):\n",
    "        animal_to_imgs[animal_name] = []\n",
    "        animal_dir = data_dir + \"/\" + animal_name + \"/\"\n",
    "        for img_name in os.listdir(animal_dir):\n",
    "            img = plt.imread(animal_dir + img_name)\n",
    "            animal_to_imgs[animal_name].append(img)\n",
    "    return animal_to_imgs\n",
    "\n",
    "def load_info():\n",
    "    df_classes = pd.read_csv(\"classes.txt\", header=None) \n",
    "    df_predicate_matrix = pd.read_csv(\"predicate-matrix-binary.txt\", header=None)\n",
    "    df_test_classes = pd.read_csv(\"testclasses.txt\", header=None)\n",
    "    df_train_classes = pd.read_csv(\"trainclasses.txt\", header=None)\n",
    "\n",
    "    animal_to_feat = {}\n",
    "    id_to_name, name_to_id = {}, {}\n",
    "    for i, c in enumerate(df_classes[0]):\n",
    "        c_name = c.split()[1]\n",
    "        id_to_name[i] = c_name\n",
    "        name_to_id[c_name] = i\n",
    "        animal_to_feat[c_name] = np.array([int(binary) for binary in df_predicate_matrix.iloc[i, 0].split()])\n",
    "\n",
    "    train_classes, test_classes = [], []\n",
    "    for c in df_train_classes[0]: train_classes.append(c.split()[0])\n",
    "    for c in df_test_classes[0]: test_classes.append(c.split()[0])\n",
    "\n",
    "    return animal_to_feat, id_to_name, name_to_id, train_classes, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_to_imgs = load_imgs(\"images_128x128\")\n",
    "animal_to_feat, id_to_name, name_to_id, train_classes, test_classes = load_info()\n",
    "all_classes = train_classes + test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def pred_class(model, img, classes):\n",
    "    s = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "    probs = np.zeros(len(classes))\n",
    "    for i, animal in enumerate(classes):\n",
    "        probs[i] = np.prod(np.abs(s - 1.0 + animal_to_feat[animal]))\n",
    "    return probs.argsort()[-1]\n",
    "\"\"\"\n",
    "def pred_class(model, img, classes):\n",
    "    eps = 1e-5\n",
    "    s = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "    probs = np.zeros(len(classes))\n",
    "    for i, animal in enumerate(classes):\n",
    "        pos = np.sum(np.log(eps + np.abs(s - 1.0 + animal_to_feat[animal])))\n",
    "        neg = np.sum(np.log(eps + np.abs(s - animal_to_feat[animal])))\n",
    "        probs[i] = pos - neg\n",
    "    return probs.argsort()[-1]\n",
    "\n",
    "def pred_features(model, img):\n",
    "    return np.round(model.predict(np.expand_dims(img, axis=0))[0]).astype(int)\n",
    "\n",
    "def predictions(model, classes, animal_to_images):\n",
    "    y_pred, y_true = [], []\n",
    "    for i, animal in enumerate(classes):\n",
    "        for img in animal_to_images[animal]:\n",
    "            y_true.append(i)\n",
    "            y_pred.append(pred_class(model, img, classes))\n",
    "    return y_pred, y_true\n",
    "\n",
    "def feature_preds(model, classes, animal_to_images):\n",
    "    y_pred, y_true = [], []\n",
    "    for animal in classes:\n",
    "        for img in animal_to_images[animal]:\n",
    "            y_true.append(animal_to_feat[animal])\n",
    "            y_pred.append(pred_features(model, img))\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title=\"\", normalize=False, cmap=plt.cm.Blues):\n",
    "    np.set_printoptions(precision=2) # auto-rounds np numbers\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True class',\n",
    "           xlabel='Predicted class')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_test = predictions(model, test_classes, animal_to_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.array(y_pred) == np.array(y_test)))\n",
    "plot_confusion_matrix(y_test, y_pred, classes=test_classes, normalize=True, title=\"Confusion matrix, model 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.mean(np.array(f_pred) == np.array(f_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_file = pd.read_csv(\"predicates.txt\", header=None)\n",
    "predicates = []\n",
    "for line in predicate_file.iloc[:,0]: predicates.append(line.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = load_model(\"models/final-model-90.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred, final_test = predictions(final_model, test_classes, animal_to_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.array(final_pred) == np.array(final_test)))\n",
    "plot_confusion_matrix(final_test, final_pred, classes=test_classes, normalize=True, title=\"Confusion matrix, model 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_train, final_true_train = feature_preds(final_model, test_classes, animal_to_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(final_feat_pred) == np.array(final_feat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texttable as tt\n",
    "table = tt.Texttable()\n",
    "table.set_cols_align([\"l\", \"r\", \"l\", \"r\", \"l\", \"r\", \"l\", \"r\", \"l\", \"r\"])\n",
    "table.set_cols_valign([\"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\"])\n",
    "table.set_cols_width([8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
    "header = [\"Feature\", \"Score\", \"Feature\", \"Score\", \"Feature\", \"Score\", \"Feature\", \"Score\", \"Feature\", \"Score\"]\n",
    "rows = [header]\n",
    "for i in range(0, len(scores), 5):\n",
    "    temp = []\n",
    "    for j in range(5): \n",
    "        temp.append(predicates[i + j])\n",
    "        temp.append(round(final_scores[i + j], 3))\n",
    "    rows.append(temp)\n",
    "table.add_rows(rows)\n",
    "print(table.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
